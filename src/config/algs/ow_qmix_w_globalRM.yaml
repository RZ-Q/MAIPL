# use epsilon greedy action selector
action_selector: "epsilon_greedy"
epsilon_start: 1.0
epsilon_finish: 0.05
epsilon_anneal_time: 50000

runner: "pref_episode"

buffer_size: 5000

# update the target network every {} episodes
target_update_interval: 200

# use the Q_Learner to train
agent_output_type: "q"
learner: "max_q_learner_w_globalRM"
double_q: True
mixer: "qmix"
mixing_embed_dim: 32
hypernet_layers: 2
hypernet_embed: 64

central_loss: 1
qmix_loss: 1
w: 0.5 # $\alpha$ in the paper
hysteretic_qmix: True # False -> CW-QMIX, True -> OW-QMIX

central_mixing_embed_dim: 256
central_action_embed: 1
central_mac: "basic_central_mac"
central_agent: "central_rnn"
central_rnn_hidden_dim: 64
central_mixer: "ff"

name: "ow_qmix_w_globalRM"

# --- global and local RM ---
run: "pref"
use_global_reward: True
use_local_reward: False  # cannot both set to false
direct_local_preference: False
# env interact
num_seed_timesteps: 10
num_unsup_timesteps: 10000
k: 6  # for unsup training
num_interact: 3000
reset_critic: True
max_feedback: 40000
reward_schedule: True
# reward model
active: "tan"  # tan sig
reward_lr: 0.0003
reward_hidden_size: 256
reward_update: 250
ensemble_size: 3
state_or_obs: True
actions_onehot: True
reward_train_batch_size: 128
loss_func: "cross_entropy"  # KL
local_acc: 0.97
# sample config
sample_episode_size: 100  # sample these episodes from buffer
sample_segment_size: 128  # sample these segmnet pairs from sampled episodes
segment_size: 10  # should not be larger than max_seq_len in episode_batch
segment_capacity: 50000  # save these segment pairs
# preference type
global_preference_type: "true_rewards"  # policy true_rewards
policy_dir: "results/models/qmix__5m_vs_6m__4__pos__2023-12-22_20-34-50/2000064"
local_preference_type: "true_indi_rewards"  # policy true_indi_rewards
local_preference_type_training: "indi_rewards"  # policy indi_rewards
# local pretrain
lamda: 0.01
lcoal_pretrain_timesteps: 200000  # 10000 for 3m, 200000 for 5mvs6m
lcoal_label_equal_thres: 0.5