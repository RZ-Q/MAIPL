# --- QMIX specific parameters ---

# use epsilon greedy action selector
action_selector: "epsilon_greedy"
epsilon_start: 1.0
epsilon_finish: 0.05
epsilon_anneal_time: 50000

runner: "pref_episode_new"

buffer_size: 5000

# update the target network every {} episodes
target_update_interval: 200

# use the Q_Learner to train
agent_output_type: "q"
learner: "pref_qmix_learner"
double_q: True
mixer: "qmix"
mixing_embed_dim: 32
hypernet_layers: 2
hypernet_embed: 64

name: "pref_qmix_new"

# --- global and local RM ---
run: "pref"
use_global_reward: True
use_local_reward: True  # cannot both set to false
direct_local_preference: False
# env interact
pretrain_timesteps: 100
num_interact: 1
max_labels: 1000000
reward_schedule: False
# reward model
active: "sig"  # tan sig
reward_lr: 0.0003
reward_hidden_size: 512
reward_update_times: 250
ensemble_size: 3
actions_onehot: True
reward_train_batch_size: 128
loss_func: "cross_entropy"  # KL cross_entropy
local_acc: 0.97
# sample config
sample_method: "uniform"
sample_batch_size: 32  # sample these episodes from buffer
add_batch_size: 10
segment_size: 10  # should not be larger than max_seq_len in episode_batch
# preference type
policy_dir: "results/models/qmix__5m_vs_6m__3__neg__2024-01-01_00-10-45/2000249"
# local pretrain
lamda: 10
lamda_decay: True
lcoal_pretrain_timesteps: 10000  # 10000 for 3m, 200000 for 5mvs6m
lcoal_label_equal_thres: 0.4