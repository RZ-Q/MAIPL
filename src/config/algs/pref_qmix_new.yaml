# --- QMIX specific parameters ---

# use epsilon greedy action selector
action_selector: "epsilon_greedy"
epsilon_start: 1.0
epsilon_finish: 0.05
epsilon_anneal_time: 50000

runner: "pref_episode_new"

buffer_size: 5000

# update the target network every {} episodes
target_update_interval: 200

# use the Q_Learner to train
agent_output_type: "q"
learner: "pref_qmix_learner_new"
double_q: True
mixer: "qmix"
mixing_embed_dim: 32
hypernet_layers: 2
hypernet_embed: 64

name: "pref_qmix_new"

# --- global and local RM ---
run: "pref"
pref_mac: "basic_mac" # Basic controller
use_global_reward: False
use_local_reward: True  # cannot both set to false
direct_local_preference: False
# env interact
pretrain_timesteps: 1000
num_interact: 1
max_labels: 1000000
reward_schedule: False
# reward model
active: "tan"  # tan sig
reward_lr: 0.0005
reward_hidden_size: 64
reward_update_times: 250
ensemble_size: 3
actions_onehot: False
reward_train_batch_size: 128
loss_func: "cross_entropy"  # KL cross_entropy
local_acc: 0.97
# sample config
sample_method: "uniform"
sample_batch_size: 32  # sample these episodes from buffer
add_batch_size: 10
segment_size: 10  # should not be larger than max_seq_len in episode_batch
# preference type
policy_dir: "results/models/qmix__5m_vs_6m__173041785__neg__2024-02-01_10-10-35/4800754"
# preference loss 
lamda: 0.005
lamda_decay: False
apply_local_timesteps: 50000  # 10000 for 3m, 200000 for 5mvs6m
lcoal_label_equal_thres: 0.4