# --- QMIX specific parameters ---

# use epsilon greedy action selector
action_selector: "epsilon_greedy"
epsilon_start: 1.0
epsilon_finish: 0.05
epsilon_anneal_time: 50000

runner: "pref_episode"

buffer_size: 5000

# update the target network every {} episodes
target_update_interval: 200

# use the Q_Learner to train
agent_output_type: "q"
learner: "pref_qmix_learner"
double_q: True
mixer: "qmix"
mixing_embed_dim: 32
hypernet_layers: 2
hypernet_embed: 64

name: "pref_qmix"

# --- global and local RM ---
run: "pref"
use_global_reward: False
use_local_reward: True  # cannot both set to false
# env interact
num_seed_timesteps: 10
num_unsup_timesteps: 100
k: 6  # for unsup training
num_interact: 150
reset_critic: True
max_feedback: 40000
reward_schedule: True
# reward model
active: "tan"  # tan sig
reward_lr: 0.0003
reward_hidden_size: 256
reward_update: 250
ensemble_size: 3
state_or_obs: True
actions_onehot: True
reward_train_batch_size: 128
loss_func: "cross_entropy"  # KL
local_acc: 0.97
# sample config
sample_episode_size: 100  # sample these episodes from buffer
sample_segment_size: 128  # sample these segmnet pairs from sampled episodes
segment_size: 10  # should not be larger than max_seq_len in episode_batch
segment_capacity: 50000  # save these segment pairs
# preference type
global_preference_type: "policy"  # policy true_rewards
policy_dir: "results/models/qmix__aloha__3__2023-12-28_22-59-10/2000020"
local_preference_type: "policy"  # policy true_indi_rewards
# local pretrain
lamda: 0.01
lcoal_pretrain_timesteps: 200000  # 10000 for 3m, 200000 for 5mvs6m
lcoal_label_equal_thres: 0.5